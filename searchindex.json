{"categories":[{"title":"syntax","uri":"https://johnhillross.com/categories/syntax/"},{"title":"themes","uri":"https://johnhillross.com/categories/themes/"}],"posts":[{"content":"快速上手YOLOv5 一、YOLOv5算法 1. 算法对比 （1）传统目标检测方法 （2）基于深度学习的目标检测算法 （2-1）Two-Stage（R-CNN/Fast R-CNN/Faster R-CNN） Two-Stage：先产生候选区域，后对目标分类与位置精修，准确率高但速度较慢\n（2-2）One-Stage（SSD/YOLO） One-Stage：直接回归目标类别与位置，速度快但准确率较低\n（2-3）One-Stage VS Two-Stage YOLO v5s：模型小、推理时间短、准确率较高\n二、环境配置 1. NVIDIA独立显卡 打开设备管理器查看是否拥有NVIDIA独立显卡：\n查看NVIDIA独立显卡是否支持CUDA：\nhttps://developer.nvidia.com/cuda-gpus\n2. 环境要求 环境 版本 地址 Visual Studio Visual Studio 2019 版本 16.11 Community 点击下载 CMake cmake-3.24.0-rc2-windows-x86_64 点击下载 OpenCV OpenCV – 3.4.16 点击下载 Anaconda 64-Bit Graphical Installer (594 MB) 点击下载 CUDA Toolkit CUDA Toolkit 11.3.1 点击下载 cuDNN cuDNN v8.2.1 (June 7th, 2021), for CUDA 11.x 点击下载 TensorRT TensorRT 8.2 GA Update 4 for x86_64 Architecture 点击下载 3. 安装Visual Studio 安装Visual Studio 2019勾选C++桌面开发工具：\n4. 安装Anaconda 添加系统环境变量：\n测试：\n换国内源：\n新建.condarc文件保存以下代码，并替换C:\\Users\\XXX下的.condarc文件\nchannels:\r- defaults\rshow_channel_urls: true\rdefault_channels:\r- http://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main\r- http://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r\r- http://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2\rcustom_channels:\rconda-forge: http://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud\rmsys2: http://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud\rbioconda: http://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud\rmenpo: http://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud\rpytorch: http://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud\rpytorch-lts: http://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud\rsimpleitk: http://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud\r5. 安装CUDA Toolkit 选择自定义\n全部安装\n添加系统环境变量：\n上述两项是安装完成Cuda后，已自动生成的环境变量的配置，我们需要自行添加的下述变量：\nCUDA_BIN_PATH: %CUDA_PATH%\\bin\nCUDA_LIB_PATH: %CUDA_PATH%\\lib\\x64\nCUDA_SDK_PATH: XXX\\NVIDIA Corporation\\CUDA Samples\\v11.1\nCUDA_SDK_BIN_PATH: %CUDA_SDK_PATH%\\bin\\win64\nCUDA_SDK_LIB_PATH: %CUDA_SDK_PATH%\\common\\lib\\x64\n并在系统变量Path中，添加一下四个信息：\n%CUDA_BIN_PATH%\n%CUDA_LIB_PATH%\n%CUDA_SDK_BIN_PATH%\n%CUDA_SDK_LIB_PATH%\n测试：\nnvcc -V\r6. 安装cuDNN 复制XXX\\cudnn-11.3-windows-x64-v8.2.1.32\\cuda全部内容到XXX\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.3\n7. 安装TensorRT 添加环境变量：\n复制XXX\\TensorRT-8.2.5.1\\lib以下所有dll文件到XXX\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.3\\bin下\n安装XXXTensorRT\\TensorRT-8.2.5.1\\python目录下tensorrt-8.2.5.1-cp39-none-win_amd64.whl：\nactivate pytorch\rpip install tensorrt-8.2.5.1-cp39-none-win_amd64.whl -i https://pypi.tuna.tsinghua.edu.cn/simple\r8. 安装OpenCV 添加环境变量：\n9. 安装CMake 添加环境变量：\n10. 安装Pytorch https://pytorch.org/get-started/locally/\n以管理员模式在命令行中运行：\nconda create -n pytorch python=3.9\ractivate pytorch\rconda install pytorch torchvision torchaudio cudatoolkit=11.3 -c pytorch\r测试：\n# cuda test\rimport torch\rprint(torch.__version__)\rx = torch.Tensor([1.0])\rxx = x.cuda()\rprint(xx)\rprint(torch.cuda.is_available())\r# cudnn test\rfrom torch.backends import cudnn\rprint(cudnn.is_acceptable(xx))\r11. 安装YOLOv5必要依赖包 下载YOLOv5源代码：\n进入XXX\\yolov5-master目录执行：\nactivate pytorch\rpip install -r requirements.txt\r测试：\nactivate pytorch\rpython detect.py\r运行结果保存在XXX\\yolov5-master\\runs\\detect\\exp下\n12. YOLOv5使用TensorRT加速 下载YOLOv5 TensorRT加速版本：\nhttps://github.com/wang-xinyu/tensorrtx\n在XXX\\tensorrtx下创建include文件夹，下载文件dirent.h放置在include文件夹中， 下载地址 https://github.com/tronkko/dirent/tree/master/include/dirent.h\n修改XXX\\tensorrtx\\yolov5目录下CMakeLists.txt内容为（按自己的安装目录修改#1 #2 #3 #4）：\ncmake_minimum_required(VERSION 2.6)\rproject(yolov5) set(OpenCV_DIR \u0026quot;D:/Environment/OpenCV/opencv/build\u0026quot;) ##1\rset(OpenCV_INCLUDE_DIRS \u0026quot;D:/Environment/OpenCV/opencv/build/include\u0026quot;) ##2\rset(OpenCV_LIBS \u0026quot;D:\\\\Environment\\\\OpenCV\\\\opencv\\\\build\\\\x64\\\\vc14\\\\lib\\\\opencv_world3416.lib\u0026quot;) ##3\rset(TRT_DIR \u0026quot;D:/Environment/TensorRT/TensorRT-8.2.5.1\u0026quot;) ##4\radd_definitions(-DAPI_EXPORTS)\radd_definitions(-std=c++11)\roption(CUDA_USE_STATIC_CUDA_RUNTIME OFF)\rset(CMAKE_CXX_STANDARD 11)\rset(CMAKE_BUILD_TYPE Debug)\rset(THREADS_PREFER_PTHREAD_FLAG ON)\rfind_package(Threads)\r# setup CUDA\rfind_package(CUDA REQUIRED)\rmessage(STATUS \u0026quot;libraries: ${CUDA_LIBRARIES}\u0026quot;)\rmessage(STATUS \u0026quot;include path: ${CUDA_INCLUDE_DIRS}\u0026quot;)\rinclude_directories(${CUDA_INCLUDE_DIRS})\r####\renable_language(CUDA) # add this line, then no need to setup cuda path in vs\r####\rinclude_directories(${PROJECT_SOURCE_DIR}/include)\rinclude_directories(${TRT_DIR}\\\\include)\rinclude_directories(D:\\\\Environment\\\\TensorRT\\\\tensorrtx\\\\include) ##5\r#find_package(OpenCV)\rinclude_directories(${OpenCV_INCLUDE_DIRS})\rinclude_directories(${OpenCV_INCLUDE_DIRS}\\\\opencv2) #6\r# -D_MWAITXINTRIN_H_INCLUDED for solving error: identifier \u0026quot;__builtin_ia32_mwaitx\u0026quot; is undefined\rset(CMAKE_CXX_FLAGS \u0026quot;${CMAKE_CXX_FLAGS} -std=c++11 -Wall -Ofast -D_MWAITXINTRIN_H_INCLUDED\u0026quot;)\r# setup opencv\rfind_package(OpenCV QUIET\rNO_MODULE\rNO_DEFAULT_PATH\rNO_CMAKE_PATH\rNO_CMAKE_ENVIRONMENT_PATH\rNO_SYSTEM_ENVIRONMENT_PATH\rNO_CMAKE_PACKAGE_REGISTRY\rNO_CMAKE_BUILDS_PATH\rNO_CMAKE_SYSTEM_PATH\rNO_CMAKE_SYSTEM_PACKAGE_REGISTRY\r)\rmessage(STATUS \u0026quot;OpenCV library status:\u0026quot;)\rmessage(STATUS \u0026quot;version: ${OpenCV_VERSION}\u0026quot;)\rmessage(STATUS \u0026quot;libraries: ${OpenCV_LIBS}\u0026quot;)\rmessage(STATUS \u0026quot;include path: ${OpenCV_INCLUDE_DIRS}\u0026quot;)\rinclude_directories(${OpenCV_INCLUDE_DIRS})\rlink_directories(${TRT_DIR}\\\\lib)\rlink_directories(${OpenCV_DIR}\\\\x64\\\\vc14\\\\lib) #8\radd_executable(yolov5 ${PROJECT_SOURCE_DIR}/calibrator.cpp yolov5 ${PROJECT_SOURCE_DIR}/yolov5.cpp ${PROJECT_SOURCE_DIR}/yololayer.cu ${PROJECT_SOURCE_DIR}/yololayer.h ${PROJECT_SOURCE_DIR}/preprocess.cu ${PROJECT_SOURCE_DIR}/preprocess.h)\rtarget_link_libraries(yolov5 \u0026quot;nvinfer\u0026quot; \u0026quot;nvinfer_plugin\u0026quot;) #9\rtarget_link_libraries(yolov5 ${OpenCV_LIBS}) #10\rtarget_link_libraries(yolov5 ${CUDA_LIBRARIES}) #11\rtarget_link_libraries(yolov5 Threads::Threads) #12\r在XXX\\tensorrtx\\yolov5目录下新建build文件夹\n运行XXX\\CMake\\bin\\cmake-gui.exe配置：\nWhere is the source code: XXX\\tensorrtx\\yolov5\nWhere to build the binaries: XXX\\tensorrtx\\yolov5\\build\n依次执行Configure -\u0026gt; Generate -\u0026gt; Open Project\n下载YOLOv5 TensorRT Win10加速版本：\nhttps://github.com/Monday-Leo/Yolov5_Tensorrt_Win10\n复制XXX\\Yolov5_Tensorrt_Win10-master\\yolov5.cpp文件内容，替换XXX\\tensorrtx\\yolov5\\yolov5.cpp内容\n修改yololayer.h文件20行中的类别数\nstatic constexpr int CLASS_NUM = 4;\t# 修改类别数\r选择重新生成yolov5项目\n复制XXX\\tensorrtx\\yolov5\\gen_wts.py到XXX\\yolov5-master目录，执行以下代码：\nactivate pytorch\rpython gen_wts.py -w yolov5s.pt -o yolov5s.wts\r复制XXX\\yolov5-master\\yolov5s.wts到XXX\\tensorrtx\\yolov5\\build\\Release下，执行以下代码：\nyolov5.exe -s yolov5s.wts yolov5s.engine s\r执行XXX\\tensorrtx\\yolov5\\build下yolov5.sln\n右键选择yolov5项目属性\n修改配置属性-\u0026gt;常规-\u0026gt;配置类型为动态库(.dll)\n修改配置属性-\u0026gt;高级-\u0026gt;目标文件扩展名为.dll\n选择重新生成yolov5项目\n复制XXX\\tensorrtx\\yolov5\\build\\Release下yolov5.dll和yolov5s.wts到XXX\\Yolov5_Tensorrt_Win10-master\n复制XXX\\yolov5-master\\data\\images到XXX\\Yolov5_Tensorrt_Win10-master\n修改python_trt.py：\nimg = cv2.imread(\u0026quot;./images/bus.jpg\u0026quot;)\r测试：\nactivate pytorch\rpython python_trt.py\r三、获取数据集 1. 公共数据集Kaggle、MakeML等： https://www.kaggle.com/datasets\nhttps://makeml.app/dataset-store\n2. 百度图片爬虫： import json\rimport itertools\rimport urllib\rimport requests\rimport os\rimport re\rimport sys\rword = input(\u0026quot;请输入关键字：\u0026quot;)\rpageNum = input(\u0026quot;爬取页面数（每个页面含有30张图片）：\u0026quot;)\rpath = \u0026quot;D:/Data/CrawlImg\u0026quot;\rif not os.path.exists(path):\ros.mkdir(path)\rurls = []\rword = urllib.parse.quote(word)\rheaders = {\r'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.131 Safari/537.36'\r}\rurl = r\u0026quot;https://image.baidu.com/search/acjson?tn=resultjson_com\u0026amp;ipn=rj\u0026amp;ct=201326592\u0026amp;is=0%2C0\u0026amp;fp=detail\u0026amp;logid=7181752122064024928\u0026amp;cl=2\u0026amp;lm=-1\u0026amp;ie=utf-8\u0026amp;oe=utf-8\u0026amp;adpicid=0\u0026amp;lpn=0\u0026amp;st=-1\u0026amp;word={word}\u0026amp;ic=0\u0026amp;hd=undefined\u0026amp;latest=undefined\u0026amp;copyright=undefined\u0026amp;s=undefined\u0026amp;se=\u0026amp;tab=0\u0026amp;width=\u0026amp;height=\u0026amp;face=undefined\u0026amp;istype=2\u0026amp;qc=\u0026amp;nc=\u0026amp;fr=\u0026amp;simics=\u0026amp;srctype=\u0026amp;bdtype=0\u0026amp;rpstart=0\u0026amp;rpnum=0\u0026amp;cs=604827192%2C704143832\u0026amp;catename=\u0026amp;force=undefined\u0026amp;album_id=\u0026amp;album_tab=\u0026amp;cardserver=\u0026amp;tabname=\u0026amp;pn={pn}\u0026amp;rn=30\u0026amp;gsm=1\u0026amp;1619190981089=\u0026quot;\rfor i in range(int(pageNum)):\rurls.append(url.format(word=word,pn=i*30))\rstr_table = {\r'_z2C$q': ':',\r'AzdH3F': '/',\r'_z\u0026amp;e3B': '.'\r}\rchar_table = {\r'w': 'a',\r'k': 'b',\r'v': 'c',\r'1': 'd',\r'j': 'e',\r'u': 'f',\r'2': 'g',\r'i': 'h',\r't': 'i',\r'3': 'j',\r'h': 'k',\r's': 'l',\r'4': 'm',\r'g': 'n',\r'5': 'o',\r'r': 'p',\r'q': 'q',\r'6': 'r',\r'f': 's',\r'p': 't',\r'7': 'u',\r'e': 'v',\r'o': 'w',\r'8': '1',\r'd': '2',\r'n': '3',\r'9': '4',\r'c': '5',\r'm': '6',\r'0': '7',\r'b': '8',\r'l': '9',\r'a': '0'\r}\ri=1\rchar_table = {ord(key): ord(value) for key, value in char_table.items()}\rfor url in urls:\rhtml=requests.get(url,headers=headers).text\ra=re.compile(r'\u0026quot;objURL\u0026quot;:\u0026quot;(.*?)\u0026quot;')\rdownURL=re.findall(a,html)\rfor t in downURL:\rfor key, value in str_table.items():\rt = t.replace(key, value)\rt=t.translate(char_table)\rprint(t)\rtry:\rhtml_1=requests.get(t)\rif str(html_1.status_code)[0]==\u0026quot;4\u0026quot;:\rprint('请求失败')\rcontinue\rexcept Exception as e:\rprint('图片链接不存在')\rcontinue\rwith open(path+\u0026quot;/\u0026quot;+str(i)+\u0026quot;.jpg\u0026quot;,'wb') as f:\rf.write(html_1.content)\ri=i+1\r3. FFmpeg视频帧截取： ffmpeg -i [video] -r [interval] -q:v [quality] -f image2 [name]%03d.jpeg\r-r 表示每一秒几帧\n-q:v表示存储jpeg的图像质量，一般2是高质量\nffmpeg -i input.mp4 -r 1 -q:v 2 -f image2 pic-%03d.jpeg\r如此，ffmpeg会把input.mp4，每隔一秒，存一张图片下来。假设有60s，那会有60张\n四、图像标注 1. Labelme图片标注工具： 下载地址https://github.com/wkentaro/labelme/releases/download/v5.0.1/Labelme.exe\nOpen Dir设置为图片所在文件夹\nEdit -\u0026gt; Change Output Dir设置lJSON格式标签保存位置\n创建数据集目录树\nAnnotation文件夹保存labelme标注格式标签\nimages文件夹存放图片\nlabels文件夹保存YOLO格式标签\nclasses.txt存储类别名，必须按顺序一行一个类别名\nlabelme标注格式为JSON需转换为YOLO格式，并划分train、val、test数据集\nlabelme to yolo\n# coding:utf-8\rimport os\rimport shutil\rimport json\rimport argparse\rfrom sklearn.model_selection import train_test_split\r'''\r1. One row per object\r2. Each row is class x_center y_center width height format.\r3. Box coordinates must be in normalized xywh format (from 0 - 1). If your boxes are in pixels, divide x_center and width by image width, and y_center and height by image height.\r4. Class numbers are zero-indexed (start from 0).\r'''\rparser = argparse.ArgumentParser()\rparser.add_argument('--img_dir', default='./datasets/CSGO2/images',type=str, help=\u0026quot;path of images\u0026quot;)\rparser.add_argument('--json_dir', default='./datasets/CSGO2/Annotations', type=str, help=\u0026quot;path of json\u0026quot;)\rparser.add_argument('--classes_path', default='./datasets/CSGO2/classes.txt', type=str, help=\u0026quot;path of classes.txt\u0026quot;)\rparser.add_argument('--txt_dir', default='./datasets/CSGO2/labels', type=str, help=\u0026quot;save path of txt\u0026quot;)\rparser.add_argument('--split_ratio', default=[0.8, 0.1, 0.1], type=str, help=\u0026quot;random split the dataset [train, val, test], default ratio is 8:1:1\u0026quot;)\rarg = parser.parse_args()\rsets = ['train', 'val', 'test']\rdef train_test_val_split_random(img_dir, json_dir, classes_path, txt_dir, split_ratio):\rratio_train = split_ratio[0]\rratio_val = split_ratio[1]\rratio_test = split_ratio[2]\rassert int(ratio_train+ratio_test+ratio_val) == 1\rimgset_path_list = {}\rimgset_path_list[0], middle_img_path = train_test_split(os.listdir(img_dir),test_size=1-ratio_train, random_state=233)\rratio=ratio_val/(1-ratio_train)\rimgset_path_list[1], imgset_path_list[2] = train_test_split(middle_img_path,test_size=ratio, random_state=233)\rprint(\u0026quot;NUMS of train:val:test = {}:{}:{}\u0026quot;.format(len(imgset_path_list[0]), len(imgset_path_list[1]), len(imgset_path_list[2])))\rfor index, set in enumerate(sets, 0):\rsplit_imgset_dir = os.path.join(img_dir, set)\rif not os.path.exists(split_imgset_dir):\ros.makedirs(split_imgset_dir)\rfor imgset_path in imgset_path_list[index]:\rshutil.copy(os.path.join(img_dir, imgset_path), split_imgset_dir)\rsplit_txt_dir = os.path.join(txt_dir, set)\rif not os.path.exists(split_txt_dir):\ros.makedirs(split_txt_dir)\rlabelme_to_yolo(split_imgset_dir, json_dir, classes_path, split_txt_dir)\rdef labelme_to_yolo(split_imgset_dir, json_dir, classes_path, split_txt_dir):\rwith open(classes_path) as f:\rclasses = f.read().strip().split()\rimg_path_list = os.listdir(split_imgset_dir)\rfor img_path in img_path_list:\rif img_path.startswith('.'):\rcontinue\rname, suffix = os.path.splitext(img_path)\rsave_path = os.path.join(split_txt_dir, name + '.txt')\rjson_path = os.path.join(json_dir, name + '.json')\rprint('processing -\u0026gt; %s' % json_path)\rlabel_dict = json.load(open(json_path, 'r'))\rheight = label_dict['imageHeight']\rwidth = label_dict['imageWidth']\rloc_info_list = label_dict['shapes']\rlabel_info_list = list()\rfor loc_info in loc_info_list:\robj_name = loc_info.get('label')\rlabel_id = classes.index(obj_name)\rloc = loc_info.get('points')\rx0, y0 = loc[0] # 左上角点\rx1, y1 = loc[1] # 右下角点\rx_center = (x0 + x1) / 2 / width\ry_center = (y0 + y1) / 2 / height\rbox_w = (abs(x1 - x0)) / width # 这里使用绝对值是因为有时候先标注的右下角点\rbox_h = (abs(y1 - y0)) / height\rassert box_w \u0026gt; 0, print((int(x0), int(y0)), (int(x1), int(y1)))\rassert box_h \u0026gt; 0\rlabel_info_list.append([str(label_id), str(x_center), str(y_center), str(box_w), str(box_h)])\rwith open(save_path, 'a') as f:\rfor label_info in label_info_list:\rlabel_str = ' '.join(label_info)\rf.write(label_str)\rf.write('\\n')\rif __name__ == '__main__':\rtrain_test_val_split_random(arg.img_dir, arg.json_dir, arg.classes_path, arg.txt_dir, arg.split_ratio)\r复制images和labels文件夹到XXX\\yolov5-master\\datasets\\XXX目录\n设置训练数据配置文件xxx.yaml即可用YOLOv5对数据集进行训练\npath设置为数据集文件夹\nnc设置为类别数\nnames设置为类别名，必须按顺序\n修改XXX\\yolov5-master\\train.py\nparser.add_argument('--weights', type=str, default=ROOT / 'yolov5s.pt', help='initial weights path')\t# 训练已有模型绝对路径，为空时即重新训练新的模型\rparser.add_argument('--data', type=str, default=ROOT / 'datasets/CSGO/csgo.yaml', help='dataset.yaml path')\t# 训练数据配置文件xxx.yaml绝对路径\rparser.add_argument('--batch-size', type=int, default=16, help='total batch size for all GPUs, -1 for autobatch')\t# 批大小，提示显存不够设置数值减少一半\rparser.add_argument('--workers', type=int, default=0, help='max dataloader workers (per RANK in DDP mode)')\t# Windows系统下设置为0\r开始训练：\nactivate pytorch\rpython train.py\r半自动辅助标注： 利用XXX\\yolov5-master\\detect.py半自动辅助标注，设置detect.py中222行为：\nparser.add_argument('--save-txt', default=True, action='store_true', help='save results to *.txt')\r运行detect.py：\nactivate pytorch\rpython detect.py\r标签结果保存在XXX\\yolov5-master\\runs\\detect\\expX\\labels中，复制labels到数据集目录下的lables中，执行以下代码即可在Annotation文件夹下生成labelme的标注格式JSON文件\nyolo to labelme\n# coding:utf-8\rimport os\rimport cv2\rimport json\rimport argparse\rimport base64\rparser = argparse.ArgumentParser()\rparser.add_argument('--img_dir', default='./datasets/CSGO2/images',type=str, help=\u0026quot;path of images\u0026quot;)\rparser.add_argument('--txt_dir', default='./datasets/CSGO2/labels', type=str, help=\u0026quot;path of txt\u0026quot;)\rparser.add_argument('--classes_path', default='./datasets/CSGO2/classes.txt', type=str, help=\u0026quot;path of classes.txt\u0026quot;)\rparser.add_argument('--json_dir', default='./datasets/CSGO2/Annotations', type=str, help=\u0026quot;save path of json\u0026quot;)\rarg = parser.parse_args()\rdef parse_tta_label(img_path, txt_path, classes_path, json_dir):\rfile_name = img_path.split('\\\\')[-1].split('.')[0]\rimagePath = img_path.split('/')[-1]\rimg = cv2.imread(img_path)\rh, w = img.shape[:2]\rwith open(img_path, 'rb') as f:\rimage = f.read()\rimage_base64 = str(base64.b64encode(image), encoding='utf-8')\rversion = '5.0.1'\rdata_dict = dict()\rdata_dict.__setitem__('version', version)\rdata_dict.__setitem__('imagePath', imagePath)\rdata_dict.__setitem__(\u0026quot;imageData\u0026quot;, image_base64)\rdata_dict.__setitem__('imageHeight', h)\rdata_dict.__setitem__('imageWidth', w)\rdata_dict.__setitem__('flags', {})\rdata_dict['shapes'] = list()\rwith open(txt_path, 'r') as f:\rlabel_info_list = f.readlines()\rfor label_info in label_info_list:\rlabel_info = label_info.strip()\rlabel_info = label_info.split(' ')\rclass_name = label_info[0]\rc_x = float(label_info[1]) * w\rc_y = float(label_info[2]) * h\rb_w = float(label_info[3]) * w\rb_h = float(label_info[4]) * h\rx1 = c_x - b_w / 2\rx2 = c_x + b_w / 2\ry1 = c_y - b_h / 2\ry2 = c_y + b_h / 2\rpoints = [[x1, y1], [x2, y2]]\rshape_type = 'rectangle'\rshape = {}\rwith open(classes_path) as f:\rclasses = f.read().strip().split()\rshape.__setitem__('label', classes[int(class_name)])\rshape.__setitem__('points', points)\rshape.__setitem__('shape_type', shape_type)\rshape.__setitem__('flags', {})\rshape.__setitem__('group_id', None)\rdata_dict['shapes'].append(shape)\rsave_json_path = os.path.join(json_dir, '%s.json' % file_name)\rjson.dump(data_dict, open(save_json_path, 'w'), indent=4)\rdef generate_labelme_prelabel(img_dir, txt_dir, classes_path, json_dir):\rimg_name_list = os.listdir(img_dir)\rfor img_name in img_name_list:\rif img_name.startswith('.'):\rcontinue\rtxt_name = img_name.replace('images','txt').replace('.jpg','.txt').replace('.jpeg','.txt').replace('.png','.txt')\rprint('processing -\u0026gt; %s' % txt_name)\rimg_path = os.path.join(img_dir, img_name)\rtxt_path = os.path.join(txt_dir, txt_name)\rparse_tta_label(img_path, txt_path, classes_path, json_dir)\rif __name__ == '__main__':\rgenerate_labelme_prelabel(arg.img_dir, arg.txt_dir, arg.classes_path, arg.json_dir)\rOpen Dir设置为图片所在文件夹\nEdit -\u0026gt; Change Output Dir设置lJSON格式标签保存位置\n利用labelme进行人工修整后，按labelme to yolo方法转换标签格式再给YOLOv5训练\n2. AI标注： 飞桨BML：\nhttps://blog.csdn.net/weixin_42217041/article/details/119250959\n五、训练平台 1. 免费算力平台： Google Colab：\n利用Google Colab训练YOLOv5\n教程：\nhttps://blog.csdn.net/djstavav/article/details/112261905\nMultCloud：\n利用MultCloud把OneDrive数据集传出到Google Colab\n教程：\nhttps://blog.csdn.net/AWhiteDongDong/article/details/107928536\n六、Demo https://github.com/johnhillross/YOLOv5_TensorRT_Win\n顺便臭不要脸要个Star⭐\n七、参考资料 https://github.com/ultralytics/yolov5\nhttps://github.com/wang-xinyu/tensorrtx\nhttps://github.com/Monday-Leo/Yolov5_Tensorrt_Win10\nhttps://blog.csdn.net/qq_41319718/article/details/123075288\n","id":0,"section":"posts","summary":"快速上手YOLOv5 一、YOLOv5算法 1. 算法对比 （1）传统目标检测方法 （2）基于深度学习的目标检测算法 （2-1）Two-Stage（R-C","tags":["YOLOv5","Quick Start"],"title":"YOLOv5 Quick Start ","uri":"https://johnhillross.com/2022/06/yolov5-quick-start/","year":"2022"},{"content":"This article offers a sample of basic Markdown syntax that can be used in Hugo content files, also it shows whether basic HTML elements are decorated with CSS in a Hugo theme.\nHeadings The following HTML \u0026lt;h1\u0026gt;—\u0026lt;h6\u0026gt; elements represent six levels of section headings. \u0026lt;h1\u0026gt; is the highest section level while \u0026lt;h6\u0026gt; is the lowest.\nH1 H2 H3 H4 H5 H6 Paragraph Xerum, quo qui aut unt expliquam qui dolut labo. Aque venitatiusda cum, voluptionse latur sitiae dolessi aut parist aut dollo enim qui voluptate ma dolestendit peritin re plis aut quas inctum laceat est volestemque commosa as cus endigna tectur, offic to cor sequas etum rerum idem sintibus eiur? Quianimin porecus evelectur, cum que nis nust voloribus ratem aut omnimi, sitatur? Quiatem. Nam, omnis sum am facea corem alique molestrunt et eos evelece arcillit ut aut eos eos nus, sin conecerem erum fuga. Ri oditatquam, ad quibus unda veliamenimin cusam et facea ipsamus es exerum sitate dolores editium rerore eost, temped molorro ratiae volorro te reribus dolorer sperchicium faceata tiustia prat.\nItatur? Quiatae cullecum rem ent aut odis in re eossequodi nonsequ idebis ne sapicia is sinveli squiatum, core et que aut hariosam ex eat.\nBlockquotes The blockquote element represents content that is quoted from another source, optionally with a citation which must be within a footer or cite element, and optionally with in-line changes such as annotations and abbreviations.\nBlockquote without attribution Tiam, ad mint andaepu dandae nostion secatur sequo quae. Note that you can use Markdown syntax within a blockquote.\nBlockquote with attribution Don\u0026rsquo;t communicate by sharing memory, share memory by communicating. — Rob Pike1\nTables Tables aren\u0026rsquo;t part of the core Markdown spec, but Hugo supports supports them out-of-the-box.\nName Age Bob 27 Alice 23 Inline Markdown within tables Inline Markdown In Table italics bold strikethrough code Code Blocks Code block with backticks html\r\u0026lt;!DOCTYPE html\u0026gt;\r\u0026lt;html lang=\u0026quot;en\u0026quot;\u0026gt;\r\u0026lt;head\u0026gt;\r\u0026lt;meta charset=\u0026quot;UTF-8\u0026quot;\u0026gt;\r\u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt;\r\u0026lt;/head\u0026gt;\r\u0026lt;body\u0026gt;\r\u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt;\r\u0026lt;/body\u0026gt;\r\u0026lt;/html\u0026gt;\rCode block indented with four spaces \u0026lt;!DOCTYPE html\u0026gt;\r\u0026lt;html lang=\u0026quot;en\u0026quot;\u0026gt;\r\u0026lt;head\u0026gt;\r\u0026lt;meta charset=\u0026quot;UTF-8\u0026quot;\u0026gt;\r\u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt;\r\u0026lt;/head\u0026gt;\r\u0026lt;body\u0026gt;\r\u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt;\r\u0026lt;/body\u0026gt;\r\u0026lt;/html\u0026gt;\rCode block with Hugo\u0026rsquo;s internal highlight shortcode \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; List Types Ordered List First item Second item Third item Unordered List List item Another item And another item Nested list Item First Sub-item Second Sub-item Other Elements — abbr, sub, sup, kbd, mark GIF is a bitmap image format.\nH2O\nXn + Yn = Zn\nPress CTRL+ALT+Delete to end the session.\nMost salamanders are nocturnal, and hunt for insects, worms, and other small creatures.\nThe above quote is excerpted from Rob Pike\u0026rsquo;s talk during Gopherfest, November 18, 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","id":1,"section":"posts","summary":"\u003cp\u003eThis article offers a sample of basic Markdown syntax that can be used in Hugo content files, also it shows whether basic HTML elements are decorated with CSS in a Hugo theme.\u003c/p\u003e","tags":["markdown","css","html","themes"],"title":"Markdown Syntax Guide","uri":"https://johnhillross.com/2019/03/markdown-syntax/","year":"2019"},{"content":"Lorem est tota propiore conpellat pectoribus de pectora summo.\nRedit teque digerit hominumque toris verebor lumina non cervice subde tollit usus habet Arctonque, furores quas nec ferunt. Quoque montibus nunc caluere tempus inhospita parcite confusaque translucet patri vestro qui optatis lumine cognoscere flos nubis! Fronde ipsamque patulos Dryopen deorum.\nExierant elisi ambit vivere dedere Duce pollice Eris modo Spargitque ferrea quos palude Rursus nulli murmur; hastile inridet ut ab gravi sententia! Nomine potitus silentia flumen, sustinet placuit petis in dilapsa erat sunt. Atria tractus malis.\nComas hunc haec pietate fetum procerum dixit Post torum vates letum Tiresia Flumen querellas Arcanaque montibus omnes Quidem et Vagus elidunt The Van de Graaf Canon\nMane refeci capiebant unda mulcebat Victa caducifer, malo vulnere contra dicere aurato, ludit regale, voca! Retorsit colit est profanae esse virescere furit nec; iaculi matertera et visa est, viribus. Divesque creatis, tecta novat collumque vulnus est, parvas. Faces illo pepulere tempus adest. Tendit flamma, ab opes virum sustinet, sidus sequendo urbis.\nIubar proles corpore raptos vero auctor imperium; sed et huic: manus caeli Lelegas tu lux. Verbis obstitit intus oblectamina fixis linguisque ausus sperare Echionides cornuaque tenent clausit possit. Omnia putatur. Praeteritae refert ausus; ferebant e primus lora nutat, vici quae mea ipse. Et iter nil spectatae vulnus haerentia iuste et exercebat, sui et.\nEurytus Hector, materna ipsumque ut Politen, nec, nate, ignari, vernum cohaesit sequitur. Vel mitis temploque vocatus, inque alis, oculos nomen non silvis corpore coniunx ne displicet illa. Crescunt non unus, vidit visa quantum inmiti flumina mortis facto sic: undique a alios vincula sunt iactata abdita! Suspenderat ego fuit tendit: luna, ante urbem Propoetides parte.\n","id":2,"section":"posts","summary":"\u003cp\u003eLorem est tota propiore conpellat pectoribus de\npectora summo.\u003c/p\u003e","tags":["markdown","text"],"title":"Placeholder Text","uri":"https://johnhillross.com/2019/03/placeholder-text/","year":"2019"},{"content":"Emoji can be enabled in a Hugo project in a number of ways.\nThe emojify function can be called directly in templates or Inline Shortcodes.\nTo enable emoji globally, set enableEmoji to true in your site’s configuration and then you can type emoji shorthand codes directly in content files; e.g.\nThe Emoji cheat sheet is a useful reference for emoji shorthand codes.\nN.B. The above steps enable Unicode Standard emoji characters and sequences in Hugo, however the rendering of these glyphs depends on the browser and the platform. To style the emoji you can either use a third party emoji font or a font stack; e.g.\n.emoji { font-family: Apple Color Emoji,Segoe UI Emoji,NotoColorEmoji,Segoe UI Symbol,Android Emoji,EmojiSymbols; }","id":3,"section":"posts","summary":"\u003cp\u003eEmoji can be enabled in a Hugo project in a number of ways.\u003c/p\u003e","tags":["emoji"],"title":"Emoji Support","uri":"https://johnhillross.com/2019/03/emoji-support/","year":"2019"}],"tags":[{"title":"css","uri":"https://johnhillross.com/tags/css/"},{"title":"emoji","uri":"https://johnhillross.com/tags/emoji/"},{"title":"html","uri":"https://johnhillross.com/tags/html/"},{"title":"index","uri":"https://johnhillross.com/tags/index/"},{"title":"markdown","uri":"https://johnhillross.com/tags/markdown/"},{"title":"Quick Start","uri":"https://johnhillross.com/tags/quick-start/"},{"title":"text","uri":"https://johnhillross.com/tags/text/"},{"title":"themes","uri":"https://johnhillross.com/tags/themes/"},{"title":"YOLOv5","uri":"https://johnhillross.com/tags/yolov5/"}]}